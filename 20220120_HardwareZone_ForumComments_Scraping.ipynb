{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a77ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "import datetime\n",
    "import re\n",
    "import csv\n",
    "from csv import writer\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa282b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# initialise empty lists \n",
    "comments_csv = []\n",
    "comment_date_csv = []\n",
    "userid_csv = []\n",
    "\n",
    "#find last page number\n",
    "page_url = '<YOUR URL LINK>'\n",
    "page = requests.get(page_url)\n",
    "\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "results = soup.find_all(\"input\", {\"class\": \"input input--number js-numberBoxTextInput input input--numberNarrow js-pageJumpPage\"})\n",
    "lastPage = (results[0]['max'])\n",
    "\n",
    "# Extracting review texts, datetime and userid\n",
    "for x in range(1,int(lastPage)+1):\n",
    "    hardware_zone_url = '<YOUR URL LINK>' #url link with removed page number \n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "\n",
    "    response = requests.get(hardware_zone_url + str(x))\n",
    "    soup1 = BeautifulSoup(response.content, 'html.parser')\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), 'html.parser')\n",
    "\n",
    "    all_comments = soup2.find_all('div',class_='bbWrapper')\n",
    "    all_dates = soup2.find_all('li', class_='u-concealed')\n",
    "    userid = soup2.find_all('h4', class_='message-name')\n",
    "    all_links = soup2.find_all('a', class_=\"link link--external fauxBlockLink-blockLink\")\n",
    "    all_links_content = soup2.find_all('div', class_='contentRow-snippet js-unfurl-desc')\n",
    "    all_links_content2 = soup2.find_all('div', class_='contentRow-minor contentRow-minor--hideLinks')\n",
    "    all_links_reviewer_name = soup2.find_all('a', class_='bbCodeBlock-sourceJump')\n",
    "    all_links_reviewer_comment = soup2.find_all('div', class_='bbCodeBlock-expandContent js-expandContent')\n",
    "    all_links_expand = soup2.find_all('div', class_='bbCodeBlock-expandLink js-expandLink')\n",
    "    all_links_external = soup2.find_all('a', class_='link link--external')\n",
    "\n",
    "    #remove unwanted information (e.g. links and other texts not commented by reviewer)\n",
    "    for i in all_links:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_content:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_content2:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_reviewer_name:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_reviewer_comment:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_expand:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_links_external:\n",
    "        i.decompose()\n",
    "\n",
    "    for i in all_comments:\n",
    "            comments = i.get_text()\n",
    "            comments2 = comments.strip()\n",
    "            comments3 = ' '.join(re.split(r'[ \\n\\t]+',comments2))\n",
    "            print(comments3)\n",
    "            comments_csv.append(comments3)\n",
    "\n",
    "    for i in all_dates:\n",
    "        all_dates1 = i.find('time')\n",
    "        if all_dates1.has_attr('datetime'):\n",
    "          print(all_dates1['datetime'])\n",
    "          comment_date_csv.append(all_dates1['datetime'])\n",
    "\n",
    "    for i in userid:\n",
    "        userid1 = i.find('a')\n",
    "        if userid1.has_attr('data-user-id'):\n",
    "            print(userid1['data-user-id'])\n",
    "            userid_csv.append(userid1['data-user-id'])\n",
    "\n",
    "    time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f516c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove first comment of review and post \n",
    "def delete_first_comment():\n",
    "    del comments_csv[0]\n",
    "    del comment_date_csv[0]\n",
    "    del userid_csv[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421369ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re  \n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "#python -m spacy download en\n",
    "#python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681fc6d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list = []\n",
    "list_nopunct = []\n",
    "\n",
    "#split sentence into individual words\n",
    "for line in comments_csv:\n",
    "    words = line.split()\n",
    "    list.append(words)\n",
    "\n",
    "#remove punctuations \n",
    "for i in list:\n",
    "    no_punct = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in i]\n",
    "    list_nopunct.append(no_punct)\n",
    "\n",
    "#remove blanks ('')\n",
    "list_nopunct2 = [[i for i in nested if i != ''] for nested in list_nopunct]\n",
    "\n",
    "#stem words\n",
    "s_stemmer = SnowballStemmer(language='english')\n",
    "stem_comments = []\n",
    "for i in list_nopunct2:\n",
    "    stem = [(s_stemmer.stem(x)) for x in i]\n",
    "    stem_comments.append(stem)\n",
    "    print(stem_comments)\n",
    "    \n",
    "#remove all stop words \n",
    "stop_comments = []\n",
    "stop_comments = [[i for i in nested if i not in nlp.Defaults.stop_words] for nested in stem_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4c4f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write every row into csv file, for export\n",
    "\n",
    "header = ['comment_word', 'userid', 'date_of_comment']\n",
    "\n",
    "with open('plh_tenyear_mop.csv', 'a', newline='') as f_object:\n",
    "    writer_object = writer(f_object)\n",
    "    writer_object.writerow(header)\n",
    "    \n",
    "    indexCount = len(userid_csv)\n",
    "    initValue = 0 \n",
    "    for user in userid_csv:\n",
    "        for comment in stop_comments[initValue]:\n",
    "            #print(comment, user,comment_date_csv[initValue])\n",
    "            writer_object.writerow([comment, user,comment_date_csv[initValue]])\n",
    "        initValue+=1\n",
    "\n",
    "    f_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad0c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count total number of items in nested list \n",
    "#to double confirm that number of rows in excel is same\n",
    "count = 0\n",
    "for listElem in list_nopunct2:\n",
    "    count += len(listElem)   \n",
    "\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
